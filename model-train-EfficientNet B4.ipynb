{"metadata":{"colab":{"name":"Model_and_train_csv.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8367732,"sourceType":"datasetVersion","datasetId":4974190},{"sourceId":8368059,"sourceType":"datasetVersion","datasetId":4974427}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install face_recognition","metadata":{"id":"7hlPaQS4e5VI","execution":{"iopub.status.busy":"2024-05-19T17:07:59.394108Z","iopub.execute_input":"2024-05-19T17:07:59.395269Z","iopub.status.idle":"2024-05-19T17:08:11.899243Z","shell.execute_reply.started":"2024-05-19T17:07:59.395228Z","shell.execute_reply":"2024-05-19T17:08:11.898098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#THis code is to check if the video is corrupted or not..\n#If the video is corrupted delete the video.\nimport glob\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nimport os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport face_recognition\n#Check if the file is corrupted or not\ndef validate_video(vid_path,train_transforms):\n      transform = train_transforms\n      count = 20\n      video_path = vid_path\n      frames = []\n      a = int(100/count)\n      first_frame = np.random.randint(0,a)\n      temp_video = video_path.split('/')[-1]\n      for i,frame in enumerate(frame_extract(video_path)):\n        frames.append(transform(frame))\n        if(len(frames) == count):\n          break\n      frames = torch.stack(frames)\n      frames = frames[:count]\n      return frames\n#extract a from from video\ndef frame_extract(path):\n  vidObj = cv2.VideoCapture(path) \n  success = 1\n  while success:\n      success, image = vidObj.read()\n      if success:\n          yield image\n\nim_size = 112\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\nvideo_fil =  glob.glob('/kaggle/input/deepfake-face-only/Celeb_fake_face_only/Celeb_fake_face_only/*.mp4')\nvideo_fil += glob.glob('/kaggle/input/deepfake-face-only/Celeb_real_face_only/Celeb_real_face_only/*.mp4')\nvideo_fil += glob.glob('/kaggle/input/deepfake-face-only/DFDC_FAKE_Face_only_data/DFDC_FAKE_Face_only_data/*.mp4')\nvideo_fil += glob.glob('/kaggle/input/deepfake-face-only/DFDC_REAL_Face_only_data/DFDC_REAL_Face_only_data/*.mp4')\nvideo_fil += glob.glob('/kaggle/input/deepfake-face-only/FF_Face_only_data/FF_Face_only_data/*.mp4')\nprint(\"Total no of videos :\" , len(video_fil))\nprint(video_fil)\ncount = 0;\nfor i in video_fil:\n  try:\n    count+=1\n    validate_video(i,train_transforms)\n  except:\n    print(\"Number of video processed: \" , count ,\" Remaining : \" , (len(video_fil) - count))\n    print(\"Corrupted video is : \" , i)\n    continue\nprint((len(video_fil) - count))","metadata":{"id":"QZ22Sj8d0JoT","execution":{"iopub.status.busy":"2024-05-19T17:08:11.901765Z","iopub.execute_input":"2024-05-19T17:08:11.902066Z","iopub.status.idle":"2024-05-19T17:09:25.670967Z","shell.execute_reply.started":"2024-05-19T17:08:11.902039Z","shell.execute_reply":"2024-05-19T17:09:25.669987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to load preprocessod video to memory\nimport json\nimport glob\nimport numpy as np\nimport cv2\nimport copy\nimport random\nvideo_files =  glob.glob('/kaggle/input/deepfake-face-only/Celeb_fake_face_only/Celeb_fake_face_only/*.mp4')\nvideo_files += glob.glob('/kaggle/input/deepfake-face-only/Celeb_real_face_only/Celeb_real_face_only/*.mp4')\nvideo_files += glob.glob('/kaggle/input/deepfake-face-only/DFDC_FAKE_Face_only_data/DFDC_FAKE_Face_only_data/*.mp4')\nvideo_files += glob.glob('/kaggle/input/deepfake-face-only/DFDC_REAL_Face_only_data/DFDC_REAL_Face_only_data/*.mp4')\nvideo_files += glob.glob('/kaggle/input/deepfake-face-only/FF_Face_only_data/FF_Face_only_data/*.mp4')\nrandom.shuffle(video_files)\nrandom.shuffle(video_files)\nframe_count = []\nfor video_file in video_files:\n  cap = cv2.VideoCapture(video_file)\n  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<100):\n    video_files.remove(video_file)\n    continue\n  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\nprint(\"frames are \" , frame_count)\nprint(\"Total no of video: \" , len(frame_count))\nprint('Average frame per video:',np.mean(frame_count))","metadata":{"id":"CEIygy8uDFXc","execution":{"iopub.status.busy":"2024-05-19T17:09:25.672268Z","iopub.execute_input":"2024-05-19T17:09:25.672620Z","iopub.status.idle":"2024-05-19T17:09:31.968619Z","shell.execute_reply.started":"2024-05-19T17:09:25.672589Z","shell.execute_reply":"2024-05-19T17:09:31.967703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the video name and labels from csv\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nimport os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport face_recognition\nclass video_dataset(Dataset):\n    def __init__(self,video_names,labels,sequence_length = 60,transform = None):\n        self.video_names = video_names\n        self.labels = labels\n        self.transform = transform\n        self.count = sequence_length\n    def __len__(self):\n        return len(self.video_names)\n    def __getitem__(self,idx):\n        video_path = self.video_names[idx]\n        frames = []\n        a = int(100/self.count)\n        first_frame = np.random.randint(0,a)\n        temp_video = video_path.split('/')[-1]\n        #print(temp_video)\n        label = self.labels.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n        if(label == 'FAKE'):\n          label = 0\n        if(label == 'REAL'):\n          label = 1\n        for i,frame in enumerate(self.frame_extract(video_path)):\n          frames.append(self.transform(frame))\n          if(len(frames) == self.count):\n            break\n        frames = torch.stack(frames)\n        frames = frames[:self.count]\n        #print(\"length:\" , len(frames), \"label\",label)\n        return frames,label\n    def frame_extract(self,path):\n      vidObj = cv2.VideoCapture(path) \n      success = 1\n      while success:\n          success, image = vidObj.read()\n          if success:\n              yield image\n#plot the image\ndef im_plot(tensor):\n    image = tensor.cpu().numpy().transpose(1,2,0)\n    b,g,r = cv2.split(image)\n    image = cv2.merge((r,g,b))\n    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n    image = image*255.0\n    plt.imshow(image.astype(int))\n    plt.show()","metadata":{"id":"OqGXNkqhDKZU","execution":{"iopub.status.busy":"2024-05-19T17:09:31.970041Z","iopub.execute_input":"2024-05-19T17:09:31.970501Z","iopub.status.idle":"2024-05-19T17:09:31.984798Z","shell.execute_reply.started":"2024-05-19T17:09:31.970466Z","shell.execute_reply":"2024-05-19T17:09:31.984007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count the number of fake and real videos\ndef number_of_real_and_fake_videos(data_list):\n  header_list = [\"file\",\"label\"]\n  lab = pd.read_csv('/kaggle/input/labels/Gobal_metadata.csv',names=header_list)\n  fake = 0\n  real = 0\n  for i in data_list:\n    temp_video = i.split('/')[-1]\n    label = lab.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n    if(label == 'FAKE'):\n      fake+=1\n    if(label == 'REAL'):\n      real+=1\n  return real,fake","metadata":{"id":"1leMozhXa5LF","execution":{"iopub.status.busy":"2024-05-19T17:09:31.987057Z","iopub.execute_input":"2024-05-19T17:09:31.987391Z","iopub.status.idle":"2024-05-19T17:09:32.000791Z","shell.execute_reply.started":"2024-05-19T17:09:31.987366Z","shell.execute_reply":"2024-05-19T17:09:31.999892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the labels and video in data loader\nimport random\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nheader_list = [\"file\",\"label\"]\nlabels = pd.read_csv('/kaggle/input/labels/Gobal_metadata.csv',names=header_list)\n#print(labels)\ntrain_videos = video_files[:int(0.8*len(video_files))]\nvalid_videos = video_files[int(0.8*len(video_files)):]\nprint(\"train : \" , len(train_videos))\nprint(\"test : \" , len(valid_videos))\n# train_videos,valid_videos = train_test_split(data,test_size = 0.2)\n# print(train_videos)\n\nprint(\"TRAIN: \", \"Real:\",number_of_real_and_fake_videos(train_videos)[0],\" Fake:\",number_of_real_and_fake_videos(train_videos)[1])\nprint(\"TEST: \", \"Real:\",number_of_real_and_fake_videos(valid_videos)[0],\" Fake:\",number_of_real_and_fake_videos(valid_videos)[1])\n\n\nim_size = 112\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\n\ntest_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\ntrain_data = video_dataset(train_videos,labels,sequence_length = 10,transform = train_transforms)\n#print(train_data)\nval_data = video_dataset(valid_videos,labels,sequence_length = 10,transform = train_transforms)\ntrain_loader = DataLoader(train_data,batch_size = 4,shuffle = True,num_workers = 4)\nvalid_loader = DataLoader(val_data,batch_size = 4,shuffle = True,num_workers = 4)\nimage,label = train_data[0]\nim_plot(image[0,:,:,:])","metadata":{"id":"sWMZn0YHDO2b","execution":{"iopub.status.busy":"2024-05-19T17:09:32.002313Z","iopub.execute_input":"2024-05-19T17:09:32.002622Z","iopub.status.idle":"2024-05-19T17:10:24.974347Z","shell.execute_reply.started":"2024-05-19T17:09:32.002587Z","shell.execute_reply":"2024-05-19T17:10:24.973521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport numpy as np\n\nclass Model(nn.Module):\n    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n        super(Model, self).__init__()\n        model = models.efficientnet_b4(pretrained=True)  # Residual Network CNN\n        self.model = nn.Sequential(*list(model.children())[:-2])\n        self.lstm = nn.LSTM(input_size=1792, hidden_size=hidden_dim, num_layers=lstm_layers, bidirectional=bidirectional)\n        self.relu = nn.LeakyReLU()\n        self.dp = nn.Dropout(0.4)\n        self.linear1 = nn.Linear(hidden_dim, num_classes)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, x):\n        batch_size, seq_length, c, h, w = x.shape\n        x = x.view(batch_size, seq_length, c, h, w)  # Restore the sequence dimension\n        fmap = self.model(x.view(-1, c, h, w))  # Process each frame individually\n        x = self.avgpool(fmap)\n        x = x.view(batch_size, seq_length, -1)  # Reshape to [batch_size, seq_length, hidden_dim]\n\n        # Permute dimensions for LSTM input\n        x_lstm = x.permute(1, 0, 2)  # Shape: [seq_length, batch_size, hidden_dim]\n        x_lstm, _ = self.lstm(x_lstm)\n\n        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=0)))\n\n\n\n\n\n\n\n\n","metadata":{"id":"UtOXSqyBDRnD","execution":{"iopub.status.busy":"2024-05-19T17:27:53.236936Z","iopub.execute_input":"2024-05-19T17:27:53.237581Z","iopub.status.idle":"2024-05-19T17:27:53.249044Z","shell.execute_reply.started":"2024-05-19T17:27:53.237549Z","shell.execute_reply":"2024-05-19T17:27:53.247890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(2).cuda()\na,b = model(torch.from_numpy(np.empty((1,20,3,112,112))).type(torch.cuda.FloatTensor))","metadata":{"id":"WYNhn10tDV90","execution":{"iopub.status.busy":"2024-05-19T17:27:56.485630Z","iopub.execute_input":"2024-05-19T17:27:56.486497Z","iopub.status.idle":"2024-05-19T17:27:57.386646Z","shell.execute_reply.started":"2024-05-19T17:27:56.486468Z","shell.execute_reply":"2024-05-19T17:27:57.385607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\nimport time\nimport os\nimport sys\nimport os\ndef train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n    model.train()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n    t = []\n    for i, (inputs, targets) in enumerate(data_loader):\n        if torch.cuda.is_available():\n            targets = targets.type(torch.cuda.LongTensor)\n            inputs = inputs.cuda()\n        _,outputs = model(inputs)\n        loss  = criterion(outputs,targets.type(torch.cuda.LongTensor))\n        acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n        losses.update(loss.item(), inputs.size(0))\n        accuracies.update(acc, inputs.size(0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        sys.stdout.write(\n                \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n                % (\n                    epoch,\n                    num_epochs,\n                    i,\n                    len(data_loader),\n                    losses.avg,\n                    accuracies.avg))\n    torch.save(model.state_dict(),'/kaggle/working/checkpoint.pt')\n    return losses.avg,accuracies.avg\ndef test(epoch,model, data_loader ,criterion):\n    print('Testing')\n    model.eval()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n    pred = []\n    true = []\n    count = 0\n    with torch.no_grad():\n        for i, (inputs, targets) in enumerate(data_loader):\n            if torch.cuda.is_available():\n                targets = targets.cuda().type(torch.cuda.FloatTensor)\n                inputs = inputs.cuda()\n            _,outputs = model(inputs)\n            loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))\n            acc = calculate_accuracy(outputs,targets.type(torch.cuda.LongTensor))\n            _,p = torch.max(outputs,1) \n            true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n            losses.update(loss.item(), inputs.size(0))\n            accuracies.update(acc, inputs.size(0))\n            sys.stdout.write(\n                    \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n                    % (\n                        i,\n                        len(data_loader),\n                        losses.avg,\n                        accuracies.avg\n                        )\n                    )\n        print('\\nAccuracy {}'.format(accuracies.avg))\n    return true,pred,losses.avg,accuracies.avg\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\ndef calculate_accuracy(outputs, targets):\n    batch_size = targets.size(0)\n\n    _, pred = outputs.topk(1, 1, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1))\n    n_correct_elems = correct.float().sum().item()\n    return 100* n_correct_elems / batch_size","metadata":{"id":"FKheLUWBDaNN","execution":{"iopub.status.busy":"2024-05-19T17:28:00.049037Z","iopub.execute_input":"2024-05-19T17:28:00.049890Z","iopub.status.idle":"2024-05-19T17:28:00.070033Z","shell.execute_reply.started":"2024-05-19T17:28:00.049845Z","shell.execute_reply":"2024-05-19T17:28:00.069136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sn\n#Output confusion matrix\ndef print_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    print('True positive = ', cm[0][0])\n    print('False positive = ', cm[0][1])\n    print('False negative = ', cm[1][0])\n    print('True negative = ', cm[1][1])\n    print('\\n')\n    df_cm = pd.DataFrame(cm, range(2), range(2))\n    sn.set(font_scale=1.4) # for label size\n    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n    plt.ylabel('Actual label', size = 20)\n    plt.xlabel('Predicted label', size = 20)\n    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n    plt.ylim([2, 0])\n    plt.show()\n    calculated_acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+ cm[1][1])\n    print(\"Calculated Accuracy\",calculated_acc*100)","metadata":{"id":"b8WneBZNfysN","execution":{"iopub.status.busy":"2024-05-19T17:28:04.872468Z","iopub.execute_input":"2024-05-19T17:28:04.873128Z","iopub.status.idle":"2024-05-19T17:28:04.881787Z","shell.execute_reply.started":"2024-05-19T17:28:04.873100Z","shell.execute_reply":"2024-05-19T17:28:04.880875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss(train_loss_avg,test_loss_avg,num_epochs):\n  loss_train = train_loss_avg\n  loss_val = test_loss_avg\n  print(num_epochs)\n  epochs = range(1,num_epochs+1)\n  plt.plot(epochs, loss_train, 'g', label='Training loss')\n  plt.plot(epochs, loss_val, 'b', label='validation loss')\n  plt.title('Training and Validation loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.legend()\n  plt.show()\ndef plot_accuracy(train_accuracy,test_accuracy,num_epochs):\n  loss_train = train_accuracy\n  loss_val = test_accuracy\n  epochs = range(1,num_epochs+1)\n  plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n  plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n  plt.title('Training and Validation accuracy')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend()\n  plt.show()","metadata":{"id":"fExJLjt2AtV9","execution":{"iopub.status.busy":"2024-05-19T17:28:08.376439Z","iopub.execute_input":"2024-05-19T17:28:08.376838Z","iopub.status.idle":"2024-05-19T17:28:08.384819Z","shell.execute_reply.started":"2024-05-19T17:28:08.376809Z","shell.execute_reply":"2024-05-19T17:28:08.384068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n#learning rate\nlr = 1e-5#0.001\n#number of epochs \nnum_epochs = 20\n\noptimizer = torch.optim.Adam(model.parameters(), lr= lr,weight_decay = 1e-5)\n\n#class_weights = torch.from_numpy(np.asarray([1,15])).type(torch.FloatTensor).cuda()\n#criterion = nn.CrossEntropyLoss(weight = class_weights).cuda()\ncriterion = nn.CrossEntropyLoss().cuda()\ntrain_loss_avg =[]\ntrain_accuracy = []\ntest_loss_avg = []\ntest_accuracy = []\nfor epoch in range(1,num_epochs+1):\n    l, acc = train_epoch(epoch,num_epochs,train_loader,model,criterion,optimizer)\n    train_loss_avg.append(l)\n    train_accuracy.append(acc)\n    true,pred,tl,t_acc = test(epoch,model,valid_loader,criterion)\n    test_loss_avg.append(tl)\n    test_accuracy.append(t_acc)\nplot_loss(train_loss_avg,test_loss_avg,len(train_loss_avg))\nplot_accuracy(train_accuracy,test_accuracy,len(train_accuracy))\nprint(confusion_matrix(true,pred))\nprint_confusion_matrix(true,pred)","metadata":{"id":"rUe1XrYnDdit","execution":{"iopub.status.busy":"2024-05-19T17:28:11.535924Z","iopub.execute_input":"2024-05-19T17:28:11.536562Z","iopub.status.idle":"2024-05-19T19:01:02.038622Z","shell.execute_reply.started":"2024-05-19T17:28:11.536531Z","shell.execute_reply":"2024-05-19T19:01:02.037577Z"},"trusted":true},"execution_count":null,"outputs":[]}]}